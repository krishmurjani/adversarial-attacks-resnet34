{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11768836,"sourceType":"datasetVersion","datasetId":7388461}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Classification with Pre-trained ResNet-34\n\nThis notebook demonstrates how to evaluate the performance of a pre-trained ResNet-34 model on a custom image dataset. We'll be assessing the model's ability to classify images from classes 401-500 that weren't part of its original training.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import transforms\nimport numpy as np\nimport json\nimport os\nfrom tqdm import tqdm\nfrom PIL import Image\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:25:43.320000Z","iopub.execute_input":"2025-05-13T18:25:43.320169Z","iopub.status.idle":"2025-05-13T18:25:52.306110Z","shell.execute_reply.started":"2025-05-13T18:25:43.320138Z","shell.execute_reply":"2025-05-13T18:25:52.305392Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Model Setup\n\nHere we load a pre-trained ResNet-34 model with weights trained on ImageNet. The model is moved to the appropriate device (GPU if available, otherwise CPU) and set to evaluation mode to disable features like dropout and batch normalization updates.","metadata":{}},{"cell_type":"code","source":"# Load the pre-trained ResNet-34 model\npretrained_model = torchvision.models.resnet34(weights='IMAGENET1K_V1')\npretrained_model = pretrained_model.to(device)\npretrained_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:25:52.306863Z","iopub.execute_input":"2025-05-13T18:25:52.307139Z","iopub.status.idle":"2025-05-13T18:25:53.482622Z","shell.execute_reply.started":"2025-05-13T18:25:52.307122Z","shell.execute_reply":"2025-05-13T18:25:53.481852Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 203MB/s]\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (4): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (5): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## Data Preprocessing\n\nTo ensure our images are processed consistently with how the model was trained, we set up the same normalization parameters used during the original ResNet training on ImageNet. \n\nThe transformation pipeline:\n1. Converts images to PyTorch tensors\n2. Normalizes pixel values using ImageNet mean and standard deviation","metadata":{}},{"cell_type":"code","source":"# Set up normalization parameters as specified in the project instructions\nmean_norms = np.array([0.485, 0.456, 0.406])\nstd_norms = np.array([0.229, 0.224, 0.225])\n\n# Create the transform pipeline\nplain_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean_norms, std=std_norms)\n])\n\n# Define dataset path\ndataset_path = \"/kaggle/input/testdata/TestDataSet\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:25:53.484327Z","iopub.execute_input":"2025-05-13T18:25:53.484549Z","iopub.status.idle":"2025-05-13T18:25:53.488642Z","shell.execute_reply.started":"2025-05-13T18:25:53.484533Z","shell.execute_reply":"2025-05-13T18:25:53.488005Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Custom Dataset Implementation\n\nWe implement a custom dataset class that manually loads images from the file system. This approach:\n\n- Scans through class directories to find all valid image files\n- Ensures proper RGB conversion for all images (some may be grayscale or have alpha channels)\n- Maps folder indices to class labels\n- Applies the necessary transformations to prepare images for the model\n\nThe dataset class follows PyTorch's Dataset interface with `__len__` and `__getitem__` methods.","metadata":{}},{"cell_type":"code","source":"# Load the dataset manually with RGB conversion\nclass ManualImageDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transform=None):\n        self.transform = transform\n        self.samples = []\n        self.classes = []\n        self.class_to_idx = {}\n        \n        # Get all valid directories (excluding hidden folders)\n        class_dirs = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d)) and not d.startswith('.')]\n        class_dirs.sort()  # Ensure consistent ordering\n        \n        # For each directory, find all images\n        for i, class_dir in enumerate(class_dirs):\n            self.classes.append(class_dir)\n            self.class_to_idx[class_dir] = i\n            \n            dir_path = os.path.join(root, class_dir)\n            for img_file in os.listdir(dir_path):\n                if img_file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff')):\n                    img_path = os.path.join(dir_path, img_file)\n                    self.samples.append((img_path, i))\n        \n        print(f\"Loaded {len(self.samples)} images across {len(self.classes)} classes\")\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        \n        # Open image with PIL and convert to RGB\n        img = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            img = self.transform(img)\n        \n        return img, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:25:53.489437Z","iopub.execute_input":"2025-05-13T18:25:53.489774Z","iopub.status.idle":"2025-05-13T18:25:53.640470Z","shell.execute_reply.started":"2025-05-13T18:25:53.489747Z","shell.execute_reply":"2025-05-13T18:25:53.639756Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Dataset Loading\n\nNow we instantiate our custom dataset and create a DataLoader. \n\n- We use our previously defined transformations\n- Set batch size to 1 for simplicity\n- Disable shuffling since we're only evaluating, not training","metadata":{}},{"cell_type":"code","source":"# Load the dataset\ndataset = ManualImageDataset(dataset_path, transform=plain_transforms)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:25:53.641285Z","iopub.execute_input":"2025-05-13T18:25:53.641510Z","iopub.status.idle":"2025-05-13T18:25:54.266707Z","shell.execute_reply.started":"2025-05-13T18:25:53.641494Z","shell.execute_reply":"2025-05-13T18:25:54.266087Z"}},"outputs":[{"name":"stdout","text":"Loaded 500 images across 100 classes\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Class Label Mapping\n\nTo interpret the model's predictions meaningfully, we need to map between:\n1. Local folder indices (0, 1, 2...)\n2. Actual ImageNet class indices (401-500)\n3. Human-readable class names\n\nWe load the class labels from a JSON file and create the necessary mappings.","metadata":{}},{"cell_type":"code","source":"# Load label_list.json for class names (401-500)\nwith open(os.path.join(dataset_path, 'labels_list.json'), 'r') as f:\n    label_list = json.load(f)\nprint(f\"Loaded label_list.json\")\n    \n# Print a sample of the label list\nif isinstance(label_list, list) and len(label_list) > 0:\n    print(f\"First few entries: {label_list[:3]}\")\n\n\n# Create class name mapping (index 401-500 -> name)\nclass_names = {}\nif isinstance(label_list, list):\n    for item in label_list:\n        if isinstance(item, str) and ': ' in item:\n            idx, name = item.split(': ', 1)\n            class_names[int(idx)] = name\n\n# Map folder indices to classes (401-500)\nfolder_to_class = {}\nfor i, folder in enumerate(dataset.classes):\n    class_idx = 401 + i\n    folder_to_class[folder] = class_idx\n    name = class_names.get(class_idx, f\"Unknown_{class_idx}\")\n    print(f\"Folder {i} ({folder}) -> Class {class_idx} ({name})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:25:54.267303Z","iopub.execute_input":"2025-05-13T18:25:54.267537Z","iopub.status.idle":"2025-05-13T18:25:54.277633Z","shell.execute_reply.started":"2025-05-13T18:25:54.267518Z","shell.execute_reply":"2025-05-13T18:25:54.277016Z"}},"outputs":[{"name":"stdout","text":"Loaded label_list.json\nFirst few entries: ['401: accordion', '402: acoustic guitar', '403: aircraft carrier']\nFolder 0 (n02672831) -> Class 401 (accordion)\nFolder 1 (n02676566) -> Class 402 (acoustic guitar)\nFolder 2 (n02687172) -> Class 403 (aircraft carrier)\nFolder 3 (n02690373) -> Class 404 (airliner)\nFolder 4 (n02692877) -> Class 405 (airship)\nFolder 5 (n02699494) -> Class 406 (altar)\nFolder 6 (n02701002) -> Class 407 (ambulance)\nFolder 7 (n02704792) -> Class 408 (amphibian)\nFolder 8 (n02708093) -> Class 409 (analog clock)\nFolder 9 (n02727426) -> Class 410 (apiary)\nFolder 10 (n02730930) -> Class 411 (apron)\nFolder 11 (n02747177) -> Class 412 (ashcan)\nFolder 12 (n02749479) -> Class 413 (assault rifle)\nFolder 13 (n02769748) -> Class 414 (backpack)\nFolder 14 (n02776631) -> Class 415 (bakery)\nFolder 15 (n02777292) -> Class 416 (balance beam)\nFolder 16 (n02782093) -> Class 417 (balloon)\nFolder 17 (n02783161) -> Class 418 (ballpoint)\nFolder 18 (n02786058) -> Class 419 (Band Aid)\nFolder 19 (n02787622) -> Class 420 (banjo)\nFolder 20 (n02788148) -> Class 421 (bannister)\nFolder 21 (n02790996) -> Class 422 (barbell)\nFolder 22 (n02791124) -> Class 423 (barber chair)\nFolder 23 (n02791270) -> Class 424 (barbershop)\nFolder 24 (n02793495) -> Class 425 (barn)\nFolder 25 (n02794156) -> Class 426 (barometer)\nFolder 26 (n02795169) -> Class 427 (barrel)\nFolder 27 (n02797295) -> Class 428 (barrow)\nFolder 28 (n02799071) -> Class 429 (baseball)\nFolder 29 (n02802426) -> Class 430 (basketball)\nFolder 30 (n02804414) -> Class 431 (bassinet)\nFolder 31 (n02804610) -> Class 432 (bassoon)\nFolder 32 (n02807133) -> Class 433 (bathing cap)\nFolder 33 (n02808304) -> Class 434 (bath towel)\nFolder 34 (n02808440) -> Class 435 (bathtub)\nFolder 35 (n02814533) -> Class 436 (beach wagon)\nFolder 36 (n02814860) -> Class 437 (beacon)\nFolder 37 (n02815834) -> Class 438 (beaker)\nFolder 38 (n02817516) -> Class 439 (bearskin)\nFolder 39 (n02823428) -> Class 440 (beer bottle)\nFolder 40 (n02823750) -> Class 441 (beer glass)\nFolder 41 (n02825657) -> Class 442 (bell cote)\nFolder 42 (n02834397) -> Class 443 (bib)\nFolder 43 (n02835271) -> Class 444 (bicycle-built-for-two)\nFolder 44 (n02837789) -> Class 445 (bikini)\nFolder 45 (n02840245) -> Class 446 (binder)\nFolder 46 (n02841315) -> Class 447 (binoculars)\nFolder 47 (n02843684) -> Class 448 (birdhouse)\nFolder 48 (n02859443) -> Class 449 (boathouse)\nFolder 49 (n02860847) -> Class 450 (bobsled)\nFolder 50 (n02865351) -> Class 451 (bolo tie)\nFolder 51 (n02869837) -> Class 452 (bonnet)\nFolder 52 (n02870880) -> Class 453 (bookcase)\nFolder 53 (n02871525) -> Class 454 (bookshop)\nFolder 54 (n02877765) -> Class 455 (bottlecap)\nFolder 55 (n02879718) -> Class 456 (bow)\nFolder 56 (n02883205) -> Class 457 (bow tie)\nFolder 57 (n02892201) -> Class 458 (brass)\nFolder 58 (n02892767) -> Class 459 (brassiere)\nFolder 59 (n02894605) -> Class 460 (breakwater)\nFolder 60 (n02895154) -> Class 461 (breastplate)\nFolder 61 (n02906734) -> Class 462 (broom)\nFolder 62 (n02909870) -> Class 463 (bucket)\nFolder 63 (n02910353) -> Class 464 (buckle)\nFolder 64 (n02916936) -> Class 465 (bulletproof vest)\nFolder 65 (n02917067) -> Class 466 (bullet train)\nFolder 66 (n02927161) -> Class 467 (butcher shop)\nFolder 67 (n02930766) -> Class 468 (cab)\nFolder 68 (n02939185) -> Class 469 (caldron)\nFolder 69 (n02948072) -> Class 470 (candle)\nFolder 70 (n02950826) -> Class 471 (cannon)\nFolder 71 (n02951358) -> Class 472 (canoe)\nFolder 72 (n02951585) -> Class 473 (can opener)\nFolder 73 (n02963159) -> Class 474 (cardigan)\nFolder 74 (n02965783) -> Class 475 (car mirror)\nFolder 75 (n02966193) -> Class 476 (carousel)\nFolder 76 (n02966687) -> Class 477 (carpenter's kit)\nFolder 77 (n02971356) -> Class 478 (carton)\nFolder 78 (n02974003) -> Class 479 (car wheel)\nFolder 79 (n02977058) -> Class 480 (cash machine)\nFolder 80 (n02978881) -> Class 481 (cassette)\nFolder 81 (n02979186) -> Class 482 (cassette player)\nFolder 82 (n02980441) -> Class 483 (castle)\nFolder 83 (n02981792) -> Class 484 (catamaran)\nFolder 84 (n02988304) -> Class 485 (CD player)\nFolder 85 (n02992211) -> Class 486 (cello)\nFolder 86 (n02992529) -> Class 487 (cellular telephone)\nFolder 87 (n02999410) -> Class 488 (chain)\nFolder 88 (n03000134) -> Class 489 (chainlink fence)\nFolder 89 (n03000247) -> Class 490 (chain mail)\nFolder 90 (n03000684) -> Class 491 (chain saw)\nFolder 91 (n03014705) -> Class 492 (chest)\nFolder 92 (n03016953) -> Class 493 (chiffonier)\nFolder 93 (n03017168) -> Class 494 (chime)\nFolder 94 (n03018349) -> Class 495 (china cabinet)\nFolder 95 (n03026506) -> Class 496 (Christmas stocking)\nFolder 96 (n03028079) -> Class 497 (church)\nFolder 97 (n03032252) -> Class 498 (cinema)\nFolder 98 (n03041632) -> Class 499 (cleaver)\nFolder 99 (n03042490) -> Class 500 (cliff dwelling)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Model Evaluation Function\n\nThis function evaluates the model's performance on our dataset:\n\n1. For each image, obtains model predictions\n2. Maps the local label to the correct ImageNet class index\n3. Checks if the true class appears in the model's top-k predictions\n4. Calculates accuracy metrics for different k values (1 and 5)\n\nTop-1 accuracy measures how often the model's best guess is correct.\nTop-5 accuracy measures how often the correct class appears in the model's top 5 guesses.","metadata":{}},{"cell_type":"code","source":"# Function to evaluate model and compute top-k accuracy\ndef evaluate_model(model, dataloader, folder_to_class, k_values=[1, 5]):\n    correct = {k: 0 for k in k_values}\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in tqdm(dataloader, desc=\"Evaluating model\"):\n            images = images.to(device)\n            folder_idx = labels.item()\n            folder_name = dataset.classes[folder_idx]\n            \n            # Get the correct class index for this folder (401-500)\n            true_class = folder_to_class[folder_name]\n            \n            # Forward pass\n            outputs = model(images)\n            \n            # Get top-k predictions\n            _, top_indices = outputs.topk(max(k_values), dim=1)\n            top_indices = top_indices.cpu().numpy()[0]\n            \n            # Check if true class is in top-k predictions\n            for k in k_values:\n                if true_class in top_indices[:k]:\n                    correct[k] += 1\n            \n            total += 1\n    \n    # Calculate accuracy percentages\n    accuracy = {k: 100 * correct[k] / total for k in k_values}\n    return accuracy, total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:26:34.606750Z","iopub.execute_input":"2025-05-13T18:26:34.607381Z","iopub.status.idle":"2025-05-13T18:26:34.612581Z","shell.execute_reply.started":"2025-05-13T18:26:34.607358Z","shell.execute_reply":"2025-05-13T18:26:34.612002Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Running the Evaluation\n\nNow we run the evaluation using our previously defined function and the pre-trained ResNet-34 model.\nWe'll measure both top-1 and top-5 accuracy on our test dataset.","metadata":{}},{"cell_type":"code","source":"# Evaluate the model\nprint(\"\\nEvaluating ResNet-34 on the test dataset...\")\naccuracy, total_images = evaluate_model(pretrained_model, dataloader, folder_to_class)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:26:38.022058Z","iopub.execute_input":"2025-05-13T18:26:38.022676Z","iopub.status.idle":"2025-05-13T18:26:44.053076Z","shell.execute_reply.started":"2025-05-13T18:26:38.022650Z","shell.execute_reply":"2025-05-13T18:26:44.052387Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating ResNet-34 on the test dataset...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating model: 100%|██████████| 500/500 [00:06<00:00, 83.00it/s] \n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Results and Saving\n\nFinally, we display the evaluation results and save them to a JSON file for future reference.\nThe JSON file will contain both the top-1 and top-5 accuracy percentages, as well as the total number of images evaluated.","metadata":{}},{"cell_type":"code","source":"# Print results\nprint(\"\\nResNet-34 Evaluation Results:\")\nprint(f\"Top-1 Accuracy: {accuracy[1]:.2f}%\")\nprint(f\"Top-5 Accuracy: {accuracy[5]:.2f}%\")\nprint(f\"Total images evaluated: {total_images}\")\n\n# Save results to file\nwith open('task1_results.json', 'w') as f:\n    json.dump({\n        'top1_accuracy': float(accuracy[1]),\n        'top5_accuracy': float(accuracy[5]),\n        'total_images': total_images\n    }, f, indent=4)\n\nprint(\"\\nTask 1 completed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T18:28:14.353739Z","iopub.execute_input":"2025-05-13T18:28:14.354318Z","iopub.status.idle":"2025-05-13T18:28:14.359680Z","shell.execute_reply.started":"2025-05-13T18:28:14.354295Z","shell.execute_reply":"2025-05-13T18:28:14.358983Z"}},"outputs":[{"name":"stdout","text":"\nResNet-34 Evaluation Results:\nTop-1 Accuracy: 76.00%\nTop-5 Accuracy: 94.20%\nTotal images evaluated: 500\n\nTask 1 completed successfully!\n","output_type":"stream"}],"execution_count":9}]}